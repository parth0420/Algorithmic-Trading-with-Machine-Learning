{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating trading signals with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the following steps:\n",
    "1. **Cross-validation in the time-series context** poses the additional challenge that train and validation sets need to respect the temporal order of the data so that we do not inadvertently train the model on data 'from the future' to predict the past and introduce lookahead bias. Scikit-learn's built-in [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) aims to accomplish this but does not work for this case where we have multiple time series, one for each ticker. We could solve this by manually subsetting the data for the appropriate train and validation periods. Alternatively, we can create a custom time-series splitter compatible with the scikit-learn Kfold interface (see resources). Here is an [example](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/utils.py) that illustrates how to do so for this case. This allows us to specify fixed `train_length` and `test_length` parameters, as well as a `lookahead` value that defines the forecast horizon and ensures an appropriate gap between the training and validation set.\n",
    "2. Now, we'll investigate if a more complex **gradient boosting** model is capable of improving the result. \n",
    "    - We'll use the LightGBM implementation because it often [outperforms](https://lightgbm.readthedocs.io/en/latest/Experiments.html) the alternative [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [CatBoost](https://catboost.ai/) libraries, especially in terms of runtime and memory footprint. There is also a recent scikit-learn [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) inspired by LightGBM. \n",
    "    - You need to define and [tune](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) several **hyperparameters** to define the model's complexity as well as constraints on the learning process (see [docs](https://lightgbm.readthedocs.io/en/latest/Parameters.html) for the full list). The goal is to adjust the model's capacity to the data while limiting the risk of overfitting so that the model learns the signal rather than the noise in the data and generalizes better to out-of-sample data. In other words, we aim to identify the most promising model settings based on an unbiased estimate of the generalization error, and then apply these settings to generate out-of-sample predictions.\n",
    "    - The **most important parameters** define the *number of iterations* or trees, and the *size of each tree*. LightGBM allows you to optimize the number of iterations by training a model for a certain number of iterations, then evaluating its performance before continuing to train the model. LightGBM permits [leaf-wise tree growth](https://lightgbm.readthedocs.io/en/latest/Features.html#leaf-wise-best-first-tree-growth) and let's you limit the size of the tree by setting the number of leaf nodes or by limiting the depth of the tree. A useful combination targets a certain number of leaves while constraining the minimum number of samples per leaf node to avoid excessive tree imbalances as well as noisy estimates due to leaf nodes with few samples. The default value of 32 of the number of leaves is a good starting point, but the min. number of data points per leaves should be significantly higher than the default of 20 - with 1m observations and 100 leave nodes, each leaf would on average contain 10K samples. \n",
    "    - A lower learning rate can help boost accuracy but requires more iterations and, thus, extends training time. See also LightGBM's [guidance](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) on parameter tuning.\n",
    "    - Finally, the parameter `feature_frac` let's you control the amount of randomization while building each tree.\n",
    "3. To **evaluate the model performance**, compare the cross-validation IC for different hyperparameter settings, computed again as the average of the Spearman rank correlation of the model predictions for each day with the actual returns. Identify the best-performing 5-10 models over a number of recent quarters, and check whether averaging their predictions further improves the result. In addition, use Alphalens to compute a summary tearsheet and inspect the spread between the botton and the top quintile of the predictions.\n",
    "4. Generate out-of-sample predictions for 2014-2016 using your preferred models (which may change for every quarter), and possibly average the result if you found this to yield more stable results. Repeat your Alphalens analysis for these signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you have a GPU, you can install the LightGBM GPU version instead of the CPU version to get a [decent speedup](https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html). To accomplish this, run\n",
    "```bash\n",
    "conda remove -n liveproject lightgbm -y\n",
    "pip install lightgbm --install-option=--gpu\n",
    "```\n",
    "You will also need to adapt the LightGBM parameter settings below to indicate that you'll be using the `device` `GPU`. A smaller bin size like 63 tends to perform better than the default 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also upload and run this notebook in [google collab](https://colab.research.google.com/notebooks/intro.ipynb), just install the packages required by the below imports, mount you Google Drive as described [here](https://buomsoo-kim.github.io/colab/2020/05/09/Colab-mounting-google-drive.md/) and update the `DATA_PATH` to the location on your Google Drive (relative to you mount path) with the `HDFStore` containing the Quandl dataset we created in the first milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:46.914879Z",
     "start_time": "2022-01-12T22:04:46.908589Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:48.010613Z",
     "start_time": "2022-01-12T22:04:47.039055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from alphalens.tears import (create_summary_tear_sheet,\n",
    "                             create_full_tear_sheet)\n",
    "\n",
    "from alphalens.utils import get_clean_factor_and_forward_returns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:49.641681Z",
     "start_time": "2022-01-12T22:04:49.635283Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:49.797504Z",
     "start_time": "2022-01-12T22:04:49.794272Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:49.935578Z",
     "start_time": "2022-01-12T22:04:49.932507Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:50.078270Z",
     "start_time": "2022-01-12T22:04:50.075653Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YEAR = 252 # days\n",
    "MONTH = 21 # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:50.308200Z",
     "start_time": "2022-01-12T22:04:50.302379Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    \"\"\"Return a formatted time string 'HH:MM:SS\n",
    "    based on a numeric time() value\"\"\"\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:53.075047Z",
     "start_time": "2022-01-12T22:04:52.111493Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (pd.read_hdf('stock_prices.h5', 'model_data')\n",
    "        .sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T22:04:53.191762Z",
     "start_time": "2022-01-12T22:04:53.076105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1402295 entries, ('A', Timestamp('2006-01-03 00:00:00')) to ('ZMH', Timestamp('2015-07-02 00:00:00'))\n",
      "Data columns (total 48 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   ret_01       1401795 non-null  float64\n",
      " 1   ret_03       1400795 non-null  float64\n",
      " 2   ret_05       1399795 non-null  float64\n",
      " 3   ret_10       1397295 non-null  float64\n",
      " 4   ret_21       1391795 non-null  float64\n",
      " 5   ret_42       1381295 non-null  float64\n",
      " 6   ret_63       1370795 non-null  float64\n",
      " 7   ret_126      1339295 non-null  float64\n",
      " 8   ret_252      1276295 non-null  float64\n",
      " 9   ret_fwd      1402295 non-null  float64\n",
      " 10  BB_UP        1392795 non-null  float64\n",
      " 11  BB_LOW       1392795 non-null  float64\n",
      " 12  BB_SQUEEZE   1392795 non-null  float64\n",
      " 13  HT           1370795 non-null  float64\n",
      " 14  SAR          1401795 non-null  float64\n",
      " 15  ADX          1388795 non-null  float64\n",
      " 16  ADXR         1382295 non-null  float64\n",
      " 17  PPO          1389795 non-null  float64\n",
      " 18  AARONOSC     1395295 non-null  float64\n",
      " 19  BOP          1402295 non-null  float64\n",
      " 20  CCI          1395795 non-null  float64\n",
      " 21  MACD         1385795 non-null  float64\n",
      " 22  MACD_SIGNAL  1385795 non-null  float64\n",
      " 23  MACD_HIST    1385795 non-null  float64\n",
      " 24  MFI          1395295 non-null  float64\n",
      " 25  RSI          1395295 non-null  float64\n",
      " 26  STOCHRSI     1387795 non-null  float64\n",
      " 27  STOCH        1393775 non-null  float64\n",
      " 28  ULTOSC       1388295 non-null  float64\n",
      " 29  WILLR        1395795 non-null  float64\n",
      " 30  AD           1402295 non-null  float64\n",
      " 31  ADOSC        1395707 non-null  float64\n",
      " 32  OBV          1402294 non-null  float64\n",
      " 33  ATR          1395295 non-null  float64\n",
      " 34  ALPHA_63     1333088 non-null  float64\n",
      " 35  MARKET_63    1333088 non-null  float64\n",
      " 36  SMB_63       1333088 non-null  float64\n",
      " 37  HML_63       1333088 non-null  float64\n",
      " 38  RMW_63       1333088 non-null  float64\n",
      " 39  CMA_63       1333088 non-null  float64\n",
      " 40  ALPHA_252    1238588 non-null  float64\n",
      " 41  MARKET_252   1238588 non-null  float64\n",
      " 42  SMB_252      1238588 non-null  float64\n",
      " 43  HML_252      1238588 non-null  float64\n",
      " 44  RMW_252      1238588 non-null  float64\n",
      " 45  CMA_252      1238588 non-null  float64\n",
      " 46  month        1402295 non-null  uint8  \n",
      " 47  weekday      1402295 non-null  uint8  \n",
      "dtypes: float64(46), uint8(2)\n",
      "memory usage: 500.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate model formulation, we assign target, features, and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.273320Z",
     "start_time": "2020-07-14T18:18:39.270541Z"
    }
   },
   "outputs": [],
   "source": [
    "target = 'ret_fwd'\n",
    "features = data.columns.drop(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.283104Z",
     "start_time": "2020-07-14T18:18:39.274213Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricals = ['month', 'weekday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See milestone 4 solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.292362Z",
     "start_time": "2020-07-14T18:18:39.284047Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultipleTimeSeriesCV:\n",
    "    \"\"\"Generates tuples of train_idx, test_idx pairs\n",
    "    Assumes the MultiIndex contains levels 'symbol' and 'date'\n",
    "    purges overlapping outcomes\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits=3,\n",
    "                 train_period_length=126,\n",
    "                 test_period_length=21,\n",
    "                 lookahead=None,\n",
    "                 date_idx='date',\n",
    "                 shuffle=False):\n",
    "        self.n_splits = n_splits\n",
    "        self.lookahead = lookahead\n",
    "        self.test_length = test_period_length\n",
    "        self.train_length = train_period_length\n",
    "        self.shuffle = shuffle\n",
    "        self.date_idx = date_idx\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        unique_dates = X.index.get_level_values(self.date_idx).unique()\n",
    "        days = sorted(unique_dates, reverse=True)\n",
    "        split_idx = []\n",
    "        for i in range(self.n_splits):\n",
    "            test_end_idx = i * self.test_length\n",
    "            test_start_idx = test_end_idx + self.test_length\n",
    "            train_end_idx = test_start_idx + self.lookahead - 1\n",
    "            train_start_idx = train_end_idx + self.train_length + self.lookahead - 1\n",
    "            split_idx.append([train_start_idx, train_end_idx,\n",
    "                              test_start_idx, test_end_idx])\n",
    "\n",
    "        dates = X.reset_index()[[self.date_idx]]\n",
    "        for train_start, train_end, test_start, test_end in split_idx:\n",
    "\n",
    "            train_idx = dates[(dates[self.date_idx] > days[train_start])\n",
    "                              & (dates.date <= days[train_end])].index\n",
    "            test_idx = dates[(dates.date > days[test_start])\n",
    "                             & (dates.date <= days[test_end])].index\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(list(train_idx))\n",
    "            yield train_idx.to_numpy(), test_idx.to_numpy()\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Lookback, lookahead and roll-forward periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use five years of training data to predict the 1-day forward returns for the following three months just as for the linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.304786Z",
     "start_time": "2020-07-14T18:18:39.293364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_length = 5 * YEAR\n",
    "test_length = 3 * MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.314385Z",
     "start_time": "2020-07-14T18:18:39.305731Z"
    }
   },
   "outputs": [],
   "source": [
    "lookahead = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create 16 quarters worth of out-of-sample predictions so we can select the past 4 quarters to select a model for a subsequent quarter with three years total of out-of-sample forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.322498Z",
     "start_time": "2020-07-14T18:18:39.315163Z"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:39.330627Z",
     "start_time": "2020-07-14T18:18:39.323343Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = MultipleTimeSeriesCV(n_splits=n_splits,\n",
    "                          test_period_length=test_length,\n",
    "                          lookahead=lookahead,\n",
    "                          train_period_length=train_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show rolling cross-validation periods - note that the folds move backwards in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:41.149600Z",
     "start_time": "2020-07-14T18:18:39.331503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 00 | # Train: 2011-11-08 - 2016-09-29 (1,260 days) | Test: 2016-09-30 - 2016-12-29 (63 days)\n",
      "Split: 01 | # Train: 2011-08-11 - 2016-06-30 (1,260 days) | Test: 2016-07-01 - 2016-09-29 (63 days)\n",
      "Split: 02 | # Train: 2011-05-16 - 2016-04-01 (1,260 days) | Test: 2016-04-04 - 2016-06-30 (63 days)\n",
      "Split: 03 | # Train: 2011-02-14 - 2015-12-30 (1,260 days) | Test: 2015-12-31 - 2016-04-01 (63 days)\n",
      "Split: 04 | # Train: 2010-11-17 - 2015-09-30 (1,260 days) | Test: 2015-10-01 - 2015-12-30 (63 days)\n",
      "Split: 05 | # Train: 2010-08-20 - 2015-07-01 (1,260 days) | Test: 2015-07-02 - 2015-09-30 (63 days)\n",
      "Split: 06 | # Train: 2010-05-25 - 2015-04-01 (1,260 days) | Test: 2015-04-02 - 2015-07-01 (63 days)\n",
      "Split: 07 | # Train: 2010-02-25 - 2014-12-30 (1,260 days) | Test: 2014-12-31 - 2015-04-01 (63 days)\n",
      "Split: 08 | # Train: 2009-11-26 - 2014-09-30 (1,260 days) | Test: 2014-10-01 - 2014-12-30 (63 days)\n",
      "Split: 09 | # Train: 2009-08-31 - 2014-07-03 (1,260 days) | Test: 2014-07-04 - 2014-09-30 (63 days)\n",
      "Split: 10 | # Train: 2009-06-03 - 2014-04-07 (1,260 days) | Test: 2014-04-08 - 2014-07-03 (63 days)\n",
      "Split: 11 | # Train: 2009-03-06 - 2014-01-08 (1,260 days) | Test: 2014-01-09 - 2014-04-07 (63 days)\n",
      "Split: 12 | # Train: 2008-12-09 - 2013-10-11 (1,260 days) | Test: 2013-10-14 - 2014-01-08 (63 days)\n",
      "Split: 13 | # Train: 2008-09-11 - 2013-07-16 (1,260 days) | Test: 2013-07-17 - 2013-10-11 (63 days)\n",
      "Split: 14 | # Train: 2008-06-16 - 2013-04-18 (1,260 days) | Test: 2013-04-19 - 2013-07-16 (63 days)\n",
      "Split: 15 | # Train: 2008-03-19 - 2013-01-21 (1,260 days) | Test: 2013-01-22 - 2013-04-18 (63 days)\n"
     ]
    }
   ],
   "source": [
    "for n_split, (train_idx, test_idx) in enumerate(cv.split(X=data)):\n",
    "    train = data.iloc[train_idx]\n",
    "    train_dates = train.index.get_level_values('date')\n",
    "    \n",
    "    test = data.iloc[test_idx]\n",
    "    test_dates = test.index.get_level_values('date')\n",
    "    \n",
    "    train_days = train.groupby(level=\"ticker\").size().value_counts().index[0]\n",
    "    train_start, train_end = train_dates.min().date(), train_dates.max().date()\n",
    "    \n",
    "    test_days = test.groupby(level=\"ticker\").size().value_counts().index[0]\n",
    "    test_start, test_end = test_dates.min().date(), test_dates.max().date()\n",
    "                                                  \n",
    "    print(f'Split: {n_split:02} | # Train: {train_start} - {train_end} ({train_days:5,.0f} days) | '\n",
    "          f'Test: {test_start} - {test_end} ({test_days} days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:06.236499Z",
     "start_time": "2020-07-14T18:20:06.231431Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fi(model):\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(),\n",
    "                      index=model.feature_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See LightGBM [docs](https://lightgbm.readthedocs.io/en/latest/Parameters.html) for details on the various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:06.542926Z",
     "start_time": "2020-07-14T18:20:06.539731Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_params = dict(boosting='gbdt',\n",
    "                   objective='regression',\n",
    "                   metric='None',\n",
    "#                    device='gpu', # uncomment when using GPU\n",
    "#                    max_bin=63,   # uncomment when using GPU\n",
    "                   verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some reasonable hyperparameter choices; feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:06.712333Z",
     "start_time": "2020-07-14T18:20:06.707884Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate_ops = [.01]\n",
    "max_depths = [5, 6, 7]\n",
    "\n",
    "# we'll set the num_leaves parameter to enable leaf-wise growth\n",
    "num_leaves_opts = [2 ** i for i in max_depths]\n",
    "feature_fraction_opts = [.6, .95]\n",
    "min_data_in_leaf_opts = [250, 750, 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:06.853468Z",
     "start_time": "2020-07-14T18:20:06.850071Z"
    }
   },
   "outputs": [],
   "source": [
    "param_names = ['learning_rate', \n",
    "               'num_leaves',\n",
    "               'feature_fraction', \n",
    "               'min_data_in_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:07.004418Z",
     "start_time": "2020-07-14T18:20:06.999498Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Parameters: 18\n"
     ]
    }
   ],
   "source": [
    "cv_params = list(product(learning_rate_ops,\n",
    "                         num_leaves_opts,\n",
    "                         feature_fraction_opts,\n",
    "                         min_data_in_leaf_opts))\n",
    "n_params = len(cv_params)\n",
    "print(f'# Parameters: {n_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly sample cv param combinations (here: 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:07.173939Z",
     "start_time": "2020-07-14T18:20:07.169745Z"
    }
   },
   "outputs": [],
   "source": [
    "cvp = np.random.choice(list(range(n_params)),\n",
    "                       size=int(n_params / 1),\n",
    "                       replace=False)\n",
    "cv_params_ = [cv_params[i] for i in cvp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of iterations is another hyperparamter to optimize: To do so, we generate predictions for a range of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:08.943920Z",
     "start_time": "2020-07-14T18:20:08.938413Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_boost_rounds = [10, 25, 50, 75, 100, 150, 200] + list(range(250, 5001, 250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setup, we'll train 18 different models for 16 folds each, using the past five years to predict the next three months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:09.419316Z",
     "start_time": "2020-07-14T18:20:09.414854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1260 | Test: 63 | Folds: 16 | Params:  18\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {train_length:3.0f} | '\n",
    "      f'Test: {test_length:2.0f} | '\n",
    "      f'Folds: {n_splits:.0f} | '\n",
    "      f'Params: {len(cv_params_):3.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:09.677976Z",
     "start_time": "2020-07-14T18:20:09.674917Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_store = 'lgb_tuning.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:20:12.428290Z",
     "start_time": "2020-07-14T18:20:12.422554Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = Path('models')\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to go: for each fold, we'll plot the best iteration and the corresponding avg. daily IC and its coefficient of variation as well as the time per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:22:24.865458Z",
     "start_time": "2020-07-14T18:23:06.676063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 |  4,500 |  1.83% |    8.4 | 103.8\n",
      " 1 |     10 | -0.11% | -122.0 | 108.3\n",
      " 2 |    750 |  1.21% |   17.6 | 145.0\n",
      " 3 |  2,750 |  4.01% |    5.5 | 127.0\n",
      " 4 |  2,500 |  3.78% |    3.8 | 145.6\n",
      " 5 |    750 |  3.80% |    3.9 | 140.1\n",
      " 6 |    500 |  1.70% |    7.6 | 143.1\n",
      " 7 |  3,000 |  2.87% |    5.3 | 149.3\n",
      " 8 |  2,500 |  3.14% |    6.1 | 138.7\n",
      " 9 |    100 |  1.12% |    8.3 | 131.2\n",
      "10 |  3,000 |  1.55% |    8.2 | 133.9\n",
      "11 |  5,000 |  3.32% |    3.1 | 148.0\n",
      "12 |  5,000 |  2.42% |    3.3 | 137.1\n",
      "13 |    100 |  1.99% |    4.0 | 134.5\n",
      "14 |     10 |  3.23% |    3.1 | 142.2\n",
      "15 |  5,000 | -1.08% |   -9.1 | 159.3\n",
      "\n",
      "  0 | 00:36:29 | 00:36:29 |  0.01 |  32 | 60% |  250 |   1.70% |  3000\n",
      " 0 |  5,000 |  1.49% |    9.9 | 198.2\n",
      " 1 |  5,000 | -0.42% |  -30.6 | 219.8\n",
      " 2 |    250 |  2.08% |    9.4 | 206.7\n",
      " 3 |  2,250 |  3.81% |    5.9 | 156.5\n",
      " 4 |     25 |  4.21% |    3.1 | 145.1\n",
      " 5 |  1,000 |  3.62% |    3.9 | 153.2\n",
      " 6 |    750 |  2.08% |    5.9 | 144.8\n",
      " 7 |  2,000 |  3.10% |    4.9 | 144.8\n",
      " 8 |  3,250 |  3.32% |    5.3 | 154.1\n",
      " 9 |     50 |  1.09% |    8.4 | 162.7\n",
      "10 |     10 |  1.73% |    6.2 | 149.1\n",
      "11 |  4,000 |  3.55% |    2.8 | 153.3\n",
      "12 |  5,000 |  2.21% |    3.7 | 130.1\n",
      "13 |     25 |  1.87% |    4.1 | 134.5\n",
      "14 |     10 |  2.88% |    4.0 | 145.0\n",
      "15 |  5,000 | -0.65% |  -14.5 | 157.7\n",
      "\n",
      "  1 | 01:19:06 | 00:42:37 |  0.01 |  32 | 60% |  750 |   1.73% |  2500\n",
      " 0 |  1,000 |  0.77% |   19.5 | 239.8\n",
      " 1 |  5,000 |  0.53% |   24.7 | 236.9\n",
      " 2 |    500 |  1.73% |   12.3 | 216.0\n",
      " 3 |  1,500 |  3.40% |    6.4 | 206.3\n",
      " 4 |    200 |  5.11% |    3.0 | 201.9\n",
      " 5 |    750 |  4.01% |    3.3 | 199.7\n",
      " 6 |    500 |  2.08% |    6.0 | 186.8\n",
      " 7 |  1,750 |  3.61% |    3.9 | 180.6\n",
      " 8 |  4,750 |  3.99% |    4.0 | 179.4\n",
      " 9 |     50 |  1.10% |    8.4 | 181.4\n",
      "10 |     50 |  1.13% |   10.9 | 179.4\n",
      "11 |  3,250 |  3.42% |    2.9 | 200.8\n",
      "12 |  1,750 |  2.68% |    3.0 | 199.9\n",
      "13 |  1,500 |  1.36% |    7.7 | 212.0\n",
      "14 |     25 |  3.69% |    3.2 | 211.4\n",
      "15 |  4,000 | -1.23% |   -7.4 | 213.8\n",
      "\n",
      "  2 | 02:13:13 | 00:54:07 |  0.01 |  64 | 60% | 1500 |   1.83% |  1500\n",
      " 0 |  4,500 |  1.16% |   12.5 | 167.4\n",
      " 1 |  4,750 |  0.80% |   17.9 | 167.2\n",
      " 2 |    500 |  2.20% |    9.5 | 169.1\n",
      " 3 |  2,000 |  3.72% |    5.7 | 166.7\n",
      " 4 |    150 |  4.48% |    3.5 | 166.8\n",
      " 5 |    750 |  4.23% |    3.3 | 168.7\n",
      " 6 |     25 |  0.82% |   15.0 | 166.3\n",
      " 7 |  2,500 |  3.42% |    4.2 | 166.1\n",
      " 8 |     50 |  4.20% |    5.0 | 164.8\n",
      " 9 |     50 |  2.37% |    3.6 | 164.4\n",
      "10 |     75 |  1.94% |    6.4 | 162.9\n",
      "11 |  2,000 |  3.28% |    2.9 | 213.6\n",
      "12 |  4,000 |  2.40% |    2.9 | 155.3\n",
      "13 |     50 |  1.71% |    4.8 | 156.7\n",
      "14 |    200 |  3.98% |    2.7 | 158.6\n",
      "15 |     10 | -1.11% |  -10.5 | 163.2\n",
      "\n",
      "  3 | 02:57:52 | 00:44:39 |  0.01 |  32 | 95% | 1500 |   1.57% |  1000\n",
      " 0 |  4,500 |  2.29% |    6.4 | 134.9\n",
      " 1 |  1,250 |  1.06% |   14.3 | 137.7\n",
      " 2 |    150 |  2.16% |    9.1 | 138.1\n",
      " 3 |  3,500 |  3.74% |    5.6 | 140.9\n",
      " 4 |    500 |  4.28% |    3.6 | 137.9\n",
      " 5 |    150 |  2.78% |    6.2 | 180.1\n",
      " 6 |     50 |  1.69% |    6.4 | 139.9\n",
      " 7 |  1,750 |  2.27% |    6.8 | 141.3\n",
      " 8 |  5,000 |  3.12% |    5.3 | 139.7\n",
      " 9 |    100 |  3.05% |    3.2 | 139.6\n",
      "10 |     75 |  1.73% |    7.5 | 139.3\n",
      "11 |  4,250 |  3.10% |    3.2 | 140.1\n",
      "12 |  5,000 |  2.27% |    3.6 | 138.5\n",
      "13 |     50 |  2.43% |    3.1 | 137.0\n",
      "14 |    100 |  4.62% |    2.1 | 138.1\n",
      "15 |  4,750 | -0.42% |  -23.3 | 139.1\n",
      "\n",
      "  4 | 03:35:35 | 00:37:43 |  0.01 |  32 | 95% |  250 |   1.62% |  3000\n",
      " 0 |  3,750 |  0.66% |   21.1 | 252.3\n",
      " 1 |  5,000 |  0.92% |   14.7 | 247.4\n",
      " 2 |  3,500 |  1.63% |   12.2 | 245.3\n",
      " 3 |  2,250 |  3.01% |    7.0 | 234.5\n",
      " 4 |    500 |  4.31% |    3.6 | 232.8\n",
      " 5 |    500 |  4.06% |    3.4 | 229.5\n",
      " 6 |    500 |  2.12% |    6.1 | 227.4\n",
      " 7 |    250 |  3.75% |    4.4 | 227.2\n",
      " 8 |  3,500 |  4.44% |    3.7 | 226.0\n",
      " 9 |     50 |  1.00% |    9.9 | 229.3\n",
      "10 |     75 |  1.89% |    6.9 | 229.6\n",
      "11 |  1,500 |  3.60% |    2.8 | 252.8\n",
      "12 |  4,500 |  2.82% |    3.1 | 257.3\n",
      "13 |     75 |  1.29% |    7.4 | 273.6\n",
      "14 |     10 |  3.20% |    3.6 | 273.3\n",
      "15 |  2,250 | -1.51% |   -6.2 | 275.9\n",
      "\n",
      "  5 | 04:40:50 | 01:05:15 |  0.01 | 128 | 60% |  750 |   1.68% |  1000\n",
      " 0 |  5,000 |  0.40% |   34.3 | 313.2\n",
      " 1 |  5,000 |  0.98% |   14.0 | 308.9\n",
      " 2 |     75 |  3.91% |    5.0 | 308.0\n",
      " 3 |     25 |  3.60% |    6.2 | 293.2\n",
      " 4 |     50 |  5.17% |    3.1 | 295.1\n",
      " 5 |    200 |  3.76% |    4.0 | 291.3\n",
      " 6 |    200 |  1.55% |    7.5 | 287.5\n",
      " 7 |     10 |  3.21% |    4.7 | 287.7\n",
      " 8 |  1,250 |  4.15% |    4.1 | 285.5\n",
      " 9 |     50 |  0.28% |   31.9 | 287.4\n",
      "10 |    500 |  1.33% |   10.4 | 291.5\n",
      "11 |  3,000 |  3.80% |    2.6 | 317.1\n",
      "12 |  5,000 |  2.38% |    3.6 | 320.1\n",
      "13 |    750 |  0.63% |   16.1 | 346.7\n",
      "14 |     50 |  3.02% |    3.7 | 419.1\n",
      "15 |  4,750 | -0.64% |  -13.5 | 453.1\n",
      "\n",
      "  6 | 06:05:57 | 01:25:06 |  0.01 | 128 | 95% |  750 |   1.64% |   500\n",
      " 0 |  5,000 |  1.79% |    7.4 | 289.0\n",
      " 1 |  5,000 |  1.23% |   10.8 | 286.1\n",
      " 2 |  3,000 |  1.50% |   13.4 | 295.8\n",
      " 3 |     25 |  3.20% |    6.9 | 287.8\n",
      " 4 |    100 |  4.44% |    3.5 | 288.2\n",
      " 5 |    500 |  3.35% |    4.4 | 270.1\n",
      " 6 |    250 |  2.89% |    4.0 | 248.0\n",
      " 7 |     25 |  3.49% |    5.1 | 249.0\n",
      " 8 |  3,250 |  3.98% |    4.2 | 249.1\n",
      " 9 |     75 | -0.24% |  -41.2 | 251.8\n",
      "10 |    750 |  1.19% |   11.2 | 258.2\n",
      "11 |  3,500 |  3.48% |    3.0 | 268.8\n",
      "12 |  2,750 |  2.10% |    4.0 | 235.9\n",
      "13 |  5,000 |  1.09% |   10.0 | 226.7\n",
      "14 |     25 |  3.82% |    2.9 | 227.8\n",
      "15 | "
     ]
    }
   ],
   "source": [
    "T = 0\n",
    "for p, param_vals in enumerate(cv_params_):\n",
    "\n",
    "    key = '/'.join([str(p) for p in param_vals])\n",
    "    model_key = '_'.join(key.split('/'))\n",
    "    params = dict(zip(param_names, param_vals))\n",
    "    params.update(base_params)\n",
    "    start = time()\n",
    "\n",
    "    # create data structures to store results\n",
    "    daily_ic_mean = np.full(shape=(len(num_boost_rounds),\n",
    "                                   n_splits),\n",
    "                            fill_value=np.nan)\n",
    "    daily_ic_std = daily_ic_mean.copy()\n",
    "\n",
    "    # train on data through 2016 to select the best model\n",
    "    for n_split, (train_idx, test_idx) in enumerate(cv.split(X=data)):\n",
    "        print(f'{n_split:2.0f} |', end=' ', flush=True)\n",
    "        iteration = time()\n",
    "\n",
    "        # select data points and create binary train set\n",
    "        train_set = data.iloc[train_idx, :]\n",
    "        lgb_train = lgb.Dataset(data=train_set.loc[:, features],\n",
    "                                label=train_set[target],\n",
    "                                categorical_feature=categoricals,\n",
    "                                free_raw_data=False)\n",
    "\n",
    "        # create test data\n",
    "        test_set = data.iloc[test_idx, :]\n",
    "        X_test = test_set.loc[:, features]\n",
    "        y_test = test_set.loc[:, target].to_frame('y_true')\n",
    "        \n",
    "        # train model for max num_boost_rounds\n",
    "        model = lgb.train(params=params,\n",
    "                          train_set=lgb_train,\n",
    "                          num_boost_round=num_boost_rounds[-1],\n",
    "                          verbose_eval=False)\n",
    "        \n",
    "        # persist model to generate out-of-sample predictions\n",
    "        model_file = model_path / f'{model_key}_{n_split}.txt'\n",
    "        model.save_model(model_file.as_posix())\n",
    "        \n",
    "        # evaluate trained model for various num_boost_rounds\n",
    "        for i, num_boost_round in enumerate(num_boost_rounds):\n",
    "            y_pred = model.predict(X_test, num_iteration=num_boost_round)\n",
    "            y_test = y_test.assign(y_pred=y_pred)\n",
    "            \n",
    "            # compute daily information coefficient\n",
    "            daily_ic = (y_test.groupby(level='date')\n",
    "                        .apply(lambda x: spearmanr(x.y_true, x.y_pred)[0]))\n",
    "\n",
    "            daily_ic_mean[i, n_split] = daily_ic.mean()\n",
    "            daily_ic_std[i, n_split] = daily_ic.std()\n",
    "\n",
    "        mu = pd.Series(daily_ic_mean[:, n_split], index=num_boost_rounds)\n",
    "        s = pd.Series(daily_ic_std[:, n_split], index=num_boost_rounds)\n",
    "        msg = f'{mu.idxmax():6,.0f} | {mu.max():6.2%} | {s[mu.idxmax()]/mu.max():6.1f} | '\n",
    "        print(msg + f'{time() - iteration:5.1f}', flush=True)\n",
    "\n",
    "    t = time() - start\n",
    "    T += t\n",
    "    \n",
    "    ic = pd.DataFrame(daily_ic_mean, index=num_boost_rounds)\n",
    "    ic_mean = ic.mean(1).max()\n",
    "    ic_mean_n = ic.mean(1).idxmax()\n",
    "\n",
    "    print(f'\\n{p:3.0f} | {format_time(T)} | {format_time(t)} | {params[\"learning_rate\"]:5.2f} | '\n",
    "          f'{params[\"num_leaves\"]:3.0f} | {params[\"feature_fraction\"]:3.0%} | '\n",
    "          f'{params[\"min_data_in_leaf\"]:4.0f} | '\n",
    "          f' {ic_mean:6.2%} | {ic_mean_n:5.0f}')\n",
    "    \n",
    "    ic.to_hdf(lgb_store, 'ic/' + key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather the daily IC values for the various models alongside the matching hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:53.595581Z",
     "start_time": "2020-07-15T02:29:53.591243Z"
    }
   },
   "outputs": [],
   "source": [
    "id_vars = ['num_iteration', 'learning_rate', 'num_leaves', 'feature_frac', 'num_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:53.852419Z",
     "start_time": "2020-07-15T02:29:53.741999Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "with pd.HDFStore('lgb_tuning.h5') as store:\n",
    "    for k in [k[1:] for k in store.keys() if k[1:].startswith('ic/')]:\n",
    "        _, lr, num_leaves, feature_frac, num_data = k.split('/')\n",
    "        results.append(store[k].assign(learning_rate=float(lr),\n",
    "                                       num_leaves=int(num_leaves),\n",
    "                                       feature_frac=float(feature_frac),\n",
    "                                       num_data=int(num_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:53.924874Z",
     "start_time": "2020-07-15T02:29:53.901439Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.concat(results).reset_index().rename(columns={'index': 'num_iteration'})\n",
    "results = pd.melt(results,\n",
    "                  id_vars=id_vars,\n",
    "                  value_name='ic',\n",
    "                  var_name='fold').apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:55.999653Z",
     "start_time": "2020-07-15T02:29:55.992744Z"
    }
   },
   "outputs": [],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:56.148277Z",
     "start_time": "2020-07-15T02:29:56.129337Z"
    }
   },
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll select the parameters that produced the 10 best daily IC values over four consecutive quarters and generate predictions for these configurations for the following quarter, using the already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:29:59.002412Z",
     "start_time": "2020-07-15T02:29:58.999251Z"
    }
   },
   "outputs": [],
   "source": [
    "param_names = id_vars[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:17.294655Z",
     "start_time": "2020-07-15T02:29:59.303966Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(X=data)):\n",
    "    if fold >= 12:\n",
    "        break\n",
    "    print(fold, end=' ', flush=True)\n",
    "    \n",
    "    # get test features and outcomes\n",
    "    test_data = data.iloc[test_idx]\n",
    "    X_test = test_data.loc[:, features]\n",
    "    fold_predictions = test_data.loc[:, target].to_frame('y_true')\n",
    "\n",
    "    # select top 10 models based on prior 12 months performance\n",
    "    best_params = (results[results.fold.isin(list(range(fold + 1, fold + 4)))]\n",
    "                   .groupby(id_vars)\n",
    "                   .ic.mean()\n",
    "                   .nlargest(n=10).to_frame('ic'))\n",
    "    \n",
    "    # generate predictions for top 10 models\n",
    "    for pos, params in enumerate(best_params.reset_index().to_dict('records')):\n",
    "        num_iteration = params['num_iteration']\n",
    "        param_vals = [params[p] for p in param_names]\n",
    "        model_key = '_'.join([str(p) for p in param_vals])\n",
    "        params = dict(zip(param_names, param_vals))\n",
    "        params.update(base_params)\n",
    "\n",
    "        # select matching model for current quarter\n",
    "        model_path = Path('models/' + model_key + f'_{fold}.txt')\n",
    "        model = lgb.Booster(model_file=model_path.as_posix())\n",
    "        \n",
    "        # predict returns based on current quarter features\n",
    "        fold_predictions[pos] = model.predict(X_test.loc[:, model.feature_name()], \n",
    "                                              num_iteration=num_iteration)\n",
    "    predictions.append(fold_predictions)\n",
    "predictions = pd.concat(predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:17.356236Z",
     "start_time": "2020-07-15T02:32:17.296031Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions.sort_index().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.112387Z",
     "start_time": "2020-07-15T02:32:17.369697Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_cols = predictions.columns.drop('y_true')\n",
    "for col in pred_cols:\n",
    "    rho, p = spearmanr(predictions.y_true, predictions[col])\n",
    "    print(f'Top {col}: {rho * 100:.2f} ({p:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of individual models to that of an averaged ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.213347Z",
     "start_time": "2020-07-15T02:32:18.113622Z"
    }
   },
   "outputs": [],
   "source": [
    "rho, p = spearmanr(predictions.y_true, predictions.loc[:, pred_cols].mean(1))\n",
    "print(f'\\nAvg: {rho * 100:.2f} ({p:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain an average IC of 3.01%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.247693Z",
     "start_time": "2020-07-15T02:32:18.214515Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('predictions.h5') as store:\n",
    "    store.put(f'gradient_boosting', predictions.loc[:, pred_cols].mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaLens Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look how the model predictions perform from an alpha factor perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.324794Z",
     "start_time": "2020-07-15T02:32:18.248986Z"
    }
   },
   "outputs": [],
   "source": [
    "t = 1\n",
    "idx = pd.IndexSlice\n",
    "factor = (predictions.loc[:, pred_cols].mean(1)\n",
    "          .sort_index()\n",
    "          .dropna()\n",
    "          .tz_localize('UTC', level='date')\n",
    "          .swaplevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.337478Z",
     "start_time": "2020-07-15T02:32:18.326100Z"
    }
   },
   "outputs": [],
   "source": [
    "factor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.344343Z",
     "start_time": "2020-07-15T02:32:18.339647Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = factor.index.unique('date')\n",
    "dates.min(), dates.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.352006Z",
     "start_time": "2020-07-15T02:32:18.345234Z"
    }
   },
   "outputs": [],
   "source": [
    "tickers = factor.index.unique('ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select next available trade prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using next available prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:18.357603Z",
     "start_time": "2020-07-15T02:32:18.352874Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trade_prices(tickers):\n",
    "    return (pd.read_hdf('stock_prices.h5', 'us_stocks')\n",
    "              .loc[idx[tickers, '2006':'2017'], 'open']\n",
    "              .unstack('ticker')\n",
    "              .sort_index()\n",
    "            .shift(-1)\n",
    "            .tz_localize('UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:32.662608Z",
     "start_time": "2020-07-15T02:32:18.358435Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trade_prices = get_trade_prices(tickers)\n",
    "trade_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get AlphaLens Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:37.016950Z",
     "start_time": "2020-07-15T02:32:32.663767Z"
    }
   },
   "outputs": [],
   "source": [
    "factor_data = get_clean_factor_and_forward_returns(factor=factor,\n",
    "                                                   prices=trade_prices,\n",
    "                                                   quantiles=10,\n",
    "                                                   periods=(1, 2, 3, 5))\n",
    "factor_data.sort_index().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Tearsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 1-day holding period, we find a meaningful (but asymmetric) spread between the top and bottom deciles; the results suggest that our model does best at identifying poor near-term performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:32:50.635196Z",
     "start_time": "2020-07-15T02:32:37.017884Z"
    }
   },
   "outputs": [],
   "source": [
    "create_summary_tear_sheet(factor_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.016px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
