{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating trading signals with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the following steps:\n",
    "1. **Cross-validation in the time-series context** poses the additional challenge that train and validation sets need to respect the temporal order of the data so that we do not inadvertently train the model on data 'from the future' to predict the past and introduce lookahead bias. Scikit-learn's built-in [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) aims to accomplish this but does not work for this case where we have multiple time series, one for each ticker. We could solve this by manually subsetting the data for the appropriate train and validation periods. Alternatively, we can create a custom time-series splitter compatible with the scikit-learn Kfold interface (see resources). Here is an [example](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/utils.py) that illustrates how to do so for this case. This allows us to specify fixed `train_length` and `test_length` parameters, as well as a `lookahead` value that defines the forecast horizon and ensures an appropriate gap between the training and validation set.\n",
    "2. Now, we'll investigate if a more complex **gradient boosting** model is capable of improving the result. \n",
    "    - We'll use the LightGBM implementation because it often [outperforms](https://lightgbm.readthedocs.io/en/latest/Experiments.html) the alternative [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [CatBoost](https://catboost.ai/) libraries, especially in terms of runtime and memory footprint. There is also a recent scikit-learn [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) inspired by LightGBM. \n",
    "    - You need to define and [tune](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) several **hyperparameters** to define the model's complexity as well as constraints on the learning process (see [docs](https://lightgbm.readthedocs.io/en/latest/Parameters.html) for the full list). The goal is to adjust the model's capacity to the data while limiting the risk of overfitting so that the model learns the signal rather than the noise in the data and generalizes better to out-of-sample data. In other words, we aim to identify the most promising model settings based on an unbiased estimate of the generalization error, and then apply these settings to generate out-of-sample predictions.\n",
    "    - The **most important parameters** define the *number of iterations* or trees, and the *size of each tree*. LightGBM allows you to optimize the number of iterations by training a model for a certain number of iterations, then evaluating its performance before continuing to train the model. LightGBM permits [leaf-wise tree growth](https://lightgbm.readthedocs.io/en/latest/Features.html#leaf-wise-best-first-tree-growth) and let's you limit the size of the tree by setting the number of leaf nodes or by limiting the depth of the tree. A useful combination targets a certain number of leaves while constraining the minimum number of samples per leaf node to avoid excessive tree imbalances as well as noisy estimates due to leaf nodes with few samples. The default value of 32 of the number of leaves is a good starting point, but the min. number of data points per leaves should be significantly higher than the default of 20 - with 1m observations and 100 leave nodes, each leaf would on average contain 10K samples. \n",
    "    - A lower learning rate can help boost accuracy but requires more iterations and, thus, extends training time. See also LightGBM's [guidance](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) on parameter tuning.\n",
    "    - Finally, the parameter `feature_frac` let's you control the amount of randomization while building each tree.\n",
    "3. To **evaluate the model performance**, compare the cross-validation IC for different hyperparameter settings, computed again as the average of the Spearman rank correlation of the model predictions for each day with the actual returns. Identify the best-performing 5-10 models over a number of recent quarters, and check whether averaging their predictions further improves the result. In addition, use Alphalens to compute a summary tearsheet and inspect the spread between the botton and the top quintile of the predictions.\n",
    "4. Generate out-of-sample predictions for 2014-2016 using your preferred models (which may change for every quarter), and possibly average the result if you found this to yield more stable results. Repeat your Alphalens analysis for these signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you have a GPU, you can install the LightGBM GPU version instead of the CPU version to get a [decent speedup](https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html). To accomplish this, run\n",
    "```bash\n",
    "conda remove -n liveproject lightgbm -y\n",
    "pip install lightgbm --install-option=--gpu\n",
    "```\n",
    "You will also need to adapt the LightGBM parameter settings below to indicate that you'll be using the `device` `GPU`. A smaller bin size like 63 tends to perform better than the default 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:37.605662Z",
     "start_time": "2020-07-14T18:18:37.603120Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:38.253938Z",
     "start_time": "2020-07-14T18:18:37.609476Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from alphalens.tears import (create_summary_tear_sheet,\n",
    "                             create_full_tear_sheet)\n",
    "\n",
    "from alphalens.utils import get_clean_factor_and_forward_returns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:38.256821Z",
     "start_time": "2020-07-14T18:18:38.255030Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:38.275287Z",
     "start_time": "2020-07-14T18:18:38.258488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:38.287002Z",
     "start_time": "2020-07-14T18:18:38.280413Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T18:18:38.317691Z",
     "start_time": "2020-07-14T18:18:38.311307Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path('..', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Lookback, lookahead and roll-forward periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaLens Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.016px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
